"""
Executor èŠ‚ç‚¹ï¼šæ ¹æ® planner çš„ next_action è°ƒç”¨å·¥å…·ã€‚
ç»è¿‡é‡æ„ï¼Œä½¿ç”¨ LLM åŠ¨æ€æå–å‚æ•°ï¼Œè€Œä¸æ˜¯ä¾èµ–è„†å¼±çš„ç¡¬ç¼–ç è§„åˆ™ã€‚
"""
from __future__ import annotations

import json
import inspect
from typing import Any, Dict

from langchain_core.utils.function_calling import convert_to_openai_tool
from langchain_openai import ChatOpenAI

from agent.state import AgentState
from tools.registry import registry
from config.settings import settings

# å»¶è¿Ÿåˆå§‹åŒ– LLM é¿å…å¯åŠ¨æ—¶é˜»å¡
_param_llm = None

def get_param_llm():
    """è·å–ç”¨äºå‚æ•°æå–çš„ LLM å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _param_llm
    if _param_llm is None:
        _param_llm = ChatOpenAI(
            model="gpt-4o-mini",
            api_key=settings.openrouter_api_key,
            base_url="https://openrouter.ai/api/v1",
            temperature=0,
            default_headers={"HTTP-Referer": "https://maxai.cc", "X-Title": "Max AI"},
        )
    return _param_llm

def get_tool_arguments(tool_func: callable, user_query: str, plan: list[str], state: AgentState) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM æ™ºèƒ½æå–å·¥å…·æ‰€éœ€çš„å‚æ•°ã€‚

    Args:
        tool_func: ç›®æ ‡å·¥å…·çš„å‡½æ•°å¯¹è±¡ã€‚
        user_query: ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢ã€‚
        plan: Planner ç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’ã€‚
        state: å½“å‰ Agent çŠ¶æ€ï¼Œç”¨äºæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚

    Returns:
        ä¸€ä¸ªåŒ…å«å·¥å…·æ‰€éœ€å‚æ•°çš„å­—å…¸ã€‚
    """
    # 1. å°†å‡½æ•°è½¬æ¢ä¸º OpenAI å·¥å…·æ ¼å¼ï¼Œä»¥ä¾¿ LLM ç†è§£å…¶ç»“æ„
    # LangChain çš„ convert_to_openai_tool åœ¨å¤„ç†æŸäº›å‡½æ•°ç­¾åæ—¶å­˜åœ¨é—®é¢˜
    # æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºä¸€ä¸ªæ›´å¯é çš„ schema
    sig = inspect.signature(tool_func)
    description = tool_func.__doc__ or f"æ‰§è¡Œ {tool_func.__name__}."
    
    parameters = {
        "type": "object",
        "properties": {},
        "required": [],
    }
    
    for param in sig.parameters.values():
        if param.name in ("state", "kwargs"):  # å¿½ç•¥ç‰¹æ®Šå‚æ•°
            continue
        
        param_info = {"type": "string"} # é»˜è®¤ä¸º string
        if param.annotation != inspect.Parameter.empty:
            if param.annotation == int:
                param_info["type"] = "integer"
            elif param.annotation == bool:
                param_info["type"] = "boolean"
            elif param.annotation == list:
                param_info["type"] = "array"
                param_info["items"] = {"type": "string"}

        parameters["properties"][param.name] = param_info
        
        if param.default == inspect.Parameter.empty:
            parameters["required"].append(param.name)

    tool_schema = {
        "name": tool_func.__name__,
        "description": description,
        "parameters": parameters,
    }

    # 2. æ„å»ºä¸€ä¸ªä¸“é—¨ç”¨äºå‚æ•°æå–çš„ LLM chain
    structured_llm = get_param_llm().with_structured_output(tool_schema)

    # 3. æ„å»º Prompt
    # æå–æœ€è¿‘çš„å‡ æ¡æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡
    recent_messages = "\n".join([f"{msg.type}: {msg.content}" for msg in state.get("messages", [])[-5:]])

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„å‚æ•°æå–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·è¯·æ±‚ã€æ‰§è¡Œè®¡åˆ’å’Œæœ€è¿‘çš„å¯¹è¯å†å²ï¼Œä¸ºç»™å®šçš„å·¥å…·æå–æ­£ç¡®çš„å‚æ•°ã€‚

    **æœ€è¿‘å¯¹è¯å†å²:**
    {recent_messages}

    **å½“å‰æ‰§è¡Œè®¡åˆ’:**
    {chr(10).join(f'- {step}' for step in plan)}

    **ç‰¹åˆ«æ³¨æ„**:
    - å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶ï¼ˆä¾‹å¦‚ `[ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: 'data/uploads/report.txt']`ï¼‰ï¼Œä½ éœ€è¦ä»è¿™ä¸ªè·¯å¾„ä¸­æå–å‡º `file_path` å‚æ•°ã€‚
    - `file_path` åº”è¯¥æ˜¯ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„ï¼Œä¾‹å¦‚ `'data/uploads/report.txt'`ã€‚

    è¯·æ ¹æ®ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œä¸ºåä¸º `{tool_func.__name__}` çš„å·¥å…·æå–å‚æ•°ã€‚
    ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å‚æ•°éƒ½è¢«å¡«å……ï¼Œå¹¶ç¬¦åˆæŒ‡å®šçš„ç±»å‹ã€‚
    """

    # 4. è°ƒç”¨ LLM å¹¶è·å–ç»“æ„åŒ–è¾“å‡º
    try:
        print(f"ğŸ¤– æ­£åœ¨ä¸ºå·¥å…· '{tool_func.__name__}' æå–å‚æ•°...")
        # LangChain çš„ with_structured_output ä¼šè‡ªåŠ¨å¤„ç† prompt å’Œ schema çš„ç»“åˆ
        response = structured_llm.invoke(prompt)
        print(f"âœ… æˆåŠŸæå–å‚æ•°: {response}")
        return response
    except Exception as e:
        print(f"âš ï¸ LLM å‚æ•°æå–å¤±è´¥: {e}")
        # é™çº§å¤„ç†ï¼šè¿”å›ä¸€ä¸ªç©ºå­—å…¸ï¼Œè®©å·¥å…·ä½¿ç”¨é»˜è®¤å€¼æˆ–æŠ¥å‘Šé”™è¯¯
        return {}


def executor_node(state: AgentState) -> dict[str, Any]:
    """
    æ‰§è¡Œ next_action ä¸­æŒ‡å®šçš„å·¥å…·ï¼Œå¹¶ä½¿ç”¨ LLM åŠ¨æ€è§£æå‚æ•°ã€‚

    Args:
        state: åŒ…å« next_action å’Œ plan çš„å½“å‰çŠ¶æ€ã€‚

    Returns:
        æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å« last_tool_outputã€‚
    """
    action = state.get("next_action", "none")
    user_query = state["messages"][-1].content if state["messages"] else ""
    plan = state.get("plan", [])

    if action == "none" or not action:
        return {
            "last_tool_output": "æ— éœ€æ‰§è¡Œå·¥å…·",
            "last_action": "none",
            "last_action_input": None,
        }

    # 1. ä»æ³¨å†Œè¡¨ä¸­è·å–å·¥å…·å‡½æ•°
    tool_func = registry.get(action)

    if not tool_func:
        available = ", ".join(registry.list_available())
        error_message = f"é”™è¯¯ï¼šå·¥å…· '{action}' æœªæ‰¾åˆ°ã€‚å¯ç”¨å·¥å…·: {available}"
        return {
            "last_tool_output": error_message,
            "last_action": action,
            "last_action_input": None,
        }

    try:
        # 2. ä½¿ç”¨ LLM æ™ºèƒ½æå–å‚æ•°ï¼Œå¹¶ä¼ å…¥å®Œæ•´çš„ state
        tool_args = get_tool_arguments(tool_func, user_query, plan, state)

        # 3. æ‰§è¡Œå·¥å…·
        print(f"ğŸš€ æ­£åœ¨æ‰§è¡Œå·¥å…·: {action}ï¼Œå‚æ•°: {tool_args}")
        output = tool_func(**tool_args)

        return {
            "last_tool_output": str(output),
            "last_action": action,
            "last_action_input": tool_args,
            # å¦‚æœå·¥å…·æ˜¯ä»£ç æ‰§è¡Œå™¨ï¼Œä¹Ÿä¼ é€’ç”Ÿæˆçš„ä»£ç 
            "generated_code": tool_args.get("code", "") if action == "code_execution" else ""
        }

    except Exception as e:
        # æ•è·å·¥å…·æ‰§è¡ŒæœŸé—´çš„ä»»ä½•å¼‚å¸¸
        error_message = f"æ‰§è¡Œå·¥å…· '{action}' æ—¶å‡ºé”™: {e}"
        print(f"âŒ {error_message}")
        import traceback
        traceback.print_exc()
        return {
            "last_tool_output": error_message,
            "last_action": action,
            "last_action_input": tool_args if 'tool_args' in locals() else "å‚æ•°æå–å¤±è´¥",
        }
