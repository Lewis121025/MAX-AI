"""FastAgent ç¼–æ’å›¾ï¼šé›¶å¹»è§‰ã€å•æ¬¡ LLMã€<800ms å“åº”ã€‚

æ–°æ¶æ„:
  1. FastPlanner (é›¶ LLM): <120ms
  2. ParallelExecutor (é›¶ LLM): <5000ms
  3. ResultPolisher (1æ¬¡ LLM): <500ms
"""

from __future__ import annotations

import time
from typing import Any, Dict

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from orchestrator.fast_planner import fast_planner, Task
from orchestrator.parallel_executor import parallel_executor
from orchestrator.result_polisher import result_polisher


def create_graph():
    """
    åˆ›å»º FastAgent ç¼–æ’å™¨
    
    æµç¨‹:
      ç”¨æˆ·è¾“å…¥ â†’ FastPlanner â†’ ParallelExecutor â†’ ResultPolisher â†’ æœ€ç»ˆç­”æ¡ˆ
      
    æ€§èƒ½:
      - å»¶è¿Ÿ: <800ms (ç®€å•) æˆ– <5s (å¤æ‚)
      - LLM è°ƒç”¨: ä»… 1 æ¬¡
      - å¹»è§‰é£é™©: 0
    """
    
    def _format_task(task: Task) -> str:
        """å°†ä»»åŠ¡è½¬æ¢æˆäººç±»å¯è¯»çš„æè¿°ã€‚"""
        if not task.params:
            return f"{task.id}: {task.tool}"

        param_pairs = []
        for key, value in task.params.items():
            # é¿å…è¾“å‡ºè¿‡é•¿çš„å‚æ•°å†…å®¹
            value_str = str(value)
            if len(value_str) > 120:
                value_str = value_str[:117] + "..."
            param_pairs.append(f"{key}={value_str}")

        params_joined = ", ".join(param_pairs)
        return f"{task.id}: {task.tool}({params_joined})"

    def fast_agent_invoke(state: Dict[str, Any]) -> Dict[str, Any]:
        """FastAgent ä¸»æµç¨‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œå…¼å®¹æ—§æ¥å£ï¼‰"""
        start_time = time.time()
        
        # æå–ç”¨æˆ·æŸ¥è¯¢
        messages = state.get("messages", [])
        if not messages:
            return {
                "messages": [AIMessage(content="é”™è¯¯ï¼šæ²¡æœ‰è¾“å…¥æ¶ˆæ¯")],
                "final_answer": "é”™è¯¯ï¼šæ²¡æœ‰è¾“å…¥æ¶ˆæ¯"
            }
        
        user_query = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])
        
        # æå–ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
        history_messages = messages[:-1] if len(messages) > 1 else []
        context = {
            "uploaded_files": state.get("uploaded_files", []),
            "history": history_messages,
            "recent_turns": history_messages[-5:] if history_messages else []  # æœ€è¿‘5è½®å¯¹è¯
        }
        
        print("=" * 60)
        print(f"ğŸš€ FastAgent å¯åŠ¨")
        print(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        if history_messages:
            print(f"ğŸ“š å†å²æ¶ˆæ¯: {len(history_messages)} æ¡")
        print("=" * 60)
        
        # é˜¶æ®µ 1: å¿«é€Ÿè§„åˆ’ (é›¶ LLM, <120ms)
        print("\nâš¡ é˜¶æ®µ 1: å¿«é€Ÿè§„åˆ’ (é›¶ LLM)")
        plan = fast_planner.plan(user_query, context)

        plan_summary = [_format_task(task) for task in plan.tasks]
        next_action = plan.tasks[0].tool if plan.tasks else "none"
        parallel_batches = plan.parallel_batches
        plan_estimated_ms = plan.total_estimated_ms
        
        # ç®€å•é—®ç­”ï¼šè·³è¿‡å·¥å…·æ‰§è¡Œï¼Œç›´æ¥ç”¨ LLM å›ç­”
        if not plan.tasks:
            print("\nğŸ’¬ æ£€æµ‹åˆ°ç®€å•é—®ç­”")
            
            if plan.requires_llm_polish and result_polisher.llm:
                # ç®€å•é—®ç­”ç›´æ¥è°ƒç”¨ LLMï¼ˆåŒ…å«å†å²ä¸Šä¸‹æ–‡ï¼‰
                try:
                    # æ„å»ºåŒ…å«å†å²çš„æ¶ˆæ¯åˆ—è¡¨
                    llm_messages = [SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ï¼Œè¯·ç®€æ´å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚èƒ½å¤Ÿè®°ä½å¹¶å‚è€ƒä¹‹å‰çš„å¯¹è¯å†…å®¹ã€‚")]
                    
                    # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘5è½®ï¼‰
                    if history_messages:
                        recent_history = history_messages[-10:]
                        for msg in recent_history:
                            if hasattr(msg, 'type'):
                                if msg.type == 'human':
                                    llm_messages.append(HumanMessage(content=msg.content))
                                elif msg.type == 'ai':
                                    llm_messages.append(AIMessage(content=msg.content))
                            elif hasattr(msg, 'content'):
                                if isinstance(msg, HumanMessage):
                                    llm_messages.append(msg)
                                elif isinstance(msg, AIMessage):
                                    llm_messages.append(msg)
                    
                    # æ·»åŠ å½“å‰æŸ¥è¯¢
                    llm_messages.append(HumanMessage(content=user_query))
                    
                    response = result_polisher.llm.invoke(llm_messages)
                    answer = response.content
                    llm_calls = 1
                except Exception as e:
                    print(f"âš ï¸ LLM è°ƒç”¨å¤±è´¥: {e}")
                    answer = f"é—®é¢˜ï¼š{user_query}\n\næŠ±æ­‰ï¼Œæ— æ³•å›ç­”æ­¤é—®é¢˜ã€‚è¯·æ£€æŸ¥ API é…ç½®æˆ–é‡è¯•ã€‚"
                    llm_calls = 0
            else:
                answer = f"é—®é¢˜ï¼š{user_query}\n\néœ€è¦é…ç½® OPENROUTER_API_KEY æ‰èƒ½å›ç­”æ­¤é—®é¢˜ã€‚"
                llm_calls = 0
            
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            return {
                "messages": state["messages"] + [AIMessage(content=answer)],
                "final_answer": answer,
                "total_time_ms": elapsed_ms,
                "llm_calls": llm_calls,
                "plan": plan_summary,
                "parallel_batches": parallel_batches,
                "next_action": next_action,
                "is_complete": True,
                "plan_estimated_ms": plan_estimated_ms
            }
        
        # é˜¶æ®µ 2: å¹¶è¡Œæ‰§è¡Œ (é›¶ LLM, <5s)
        print(f"\nâš¡ é˜¶æ®µ 2: å¹¶è¡Œæ‰§è¡Œ (é›¶ LLM)")
        print(f"ğŸ“Š ä»»åŠ¡æ•°: {len(plan.tasks)}")
        print(f"ğŸ“¦ æ‰¹æ¬¡æ•°: {len(plan.parallel_batches)}")
        
        results = parallel_executor.execute(plan)
        
        # ç»Ÿè®¡æˆåŠŸç‡
        success_count = sum(1 for r in results.values() if r.success)
        total_count = len(results)
        
        # é˜¶æ®µ 3: ç»“æœæ¶¦è‰² (ä»… 1 æ¬¡ LLM, <500ms)
        print(f"\nâš¡ é˜¶æ®µ 3: ç»“æœæ¶¦è‰²")
        
        if plan.requires_llm_polish and result_polisher.llm:
            print("  ä½¿ç”¨ LLM æ¶¦è‰²")
            # ä¼ é€’å†å²æ¶ˆæ¯ç”¨äºä¸Šä¸‹æ–‡è®°å¿†
            answer = result_polisher.polish(user_query, plan, results, history_messages=history_messages)
            llm_calls = 1
        else:
            print("  ä½¿ç”¨é™çº§æ ¼å¼åŒ–")
            # ç®€å•ä»»åŠ¡æˆ– LLM æœªé…ç½®ï¼šç›´æ¥æ ¼å¼åŒ–
            answer = result_polisher._fallback_format(user_query, results)
            llm_calls = 0
        
        # å®Œæˆ
        elapsed_ms = int((time.time() - start_time) * 1000)
        
        print("\n" + "=" * 60)
        print(f"âœ… FastAgent å®Œæˆ")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_ms}ms")
        print(f"ğŸ“Š æˆåŠŸç‡: {success_count}/{total_count}")
        print(f"ğŸ§  LLM è°ƒç”¨: 1 æ¬¡ (ä»…æ¶¦è‰²)")
        print("=" * 60)
        
        return {
            "messages": state["messages"] + [AIMessage(content=answer)],
            "final_answer": answer,
            "total_time_ms": elapsed_ms,
            "llm_calls": llm_calls,
            "success_rate": f"{success_count}/{total_count}",
            "plan": plan_summary,
            "parallel_batches": parallel_batches,
            "next_action": next_action,
            "is_complete": True,
            "plan_estimated_ms": plan_estimated_ms,
            "tool_results": {
                task_id: {
                    "tool": result.tool,
                    "success": result.success,
                    "error": result.error if not result.success else None,
                    "output_preview": str(result.output)[:200] if result.output else None,
                    "elapsed_ms": result.elapsed_ms
                }
                for task_id, result in results.items()
            }
        }
    
    # è¿”å›ç®€å•çš„è°ƒç”¨å™¨
    class FastGraph:
        def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
            return fast_agent_invoke(state)
        
        def stream(self, state: Dict[str, Any]):
            """æµå¼ç‰ˆæœ¬ï¼Œå…¼å®¹æ—§æ¥å£"""
            # æ‰§è¡Œä¸»æµç¨‹
            result = fast_agent_invoke(state)
            
            # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆå®é™…æ˜¯æ‰¹é‡è¿”å›ï¼‰
            yield {"fast_agent": result}
    
    return FastGraph()
